{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "UpIvX1LjnkYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74Hezc4VnQx2"
      },
      "outputs": [],
      "source": [
        "# train_yolov8_advanced.py\n",
        "from ultralytics import YOLO\n",
        "import torch\n",
        "\n",
        "# GPU status\n",
        "print(\" Torch version:\", torch.__version__)\n",
        "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "\n",
        "\n",
        "# Load model (yolov8n/s/m/l/x, or custom .pt checkpoint)\n",
        "model = YOLO(\"yolov8n.pt\")  # I am using 8n, can change to 8s or 8m\n",
        "\n",
        "# Train the model\n",
        "model.train(\n",
        "    data=\"/content/drive/MyDrive/detection/foodie/data.yaml\",        # Path is relative to my 'detection' folder\n",
        "    epochs=150,                     # deeper for better convergence\n",
        "    imgsz=640,                      # Image size\n",
        "    batch=8,                        # Adjust based on memory (MPS = 8–16 is safe)\n",
        "    device=0,                   # else MPS for Apple Silicon GPU if running locally on mac(that is what I was doing earlier)\n",
        "\n",
        "    # Advanced options\n",
        "    optimizer=\"AdamW\",              # Better than SGD for some datasets\n",
        "    lr0=0.001,                      # Initial learning rate\n",
        "    lrf=0.01,                       # Final learning rate fraction\n",
        "    #momentum=0.937,                  #Only used with SGD\n",
        "    weight_decay=0.001,            # Regularization\n",
        "    warmup_epochs=3,               # Warm-up to stabilize early training\n",
        "    #warmup_bias_lr=0.05,\n",
        "\n",
        "    # Augmentations\n",
        "    hsv_h=0.015,                   # Image hue augmentation\n",
        "    hsv_s=0.7,\n",
        "    hsv_v=0.4,\n",
        "    degrees=0.0,\n",
        "    translate=0.1,\n",
        "    scale=0.5,\n",
        "    shear=0.0,\n",
        "    perspective=0.0,\n",
        "    flipud=0.0,\n",
        "    mosaic=1.0,\n",
        "    mixup=0.2,\n",
        "    erasing=0.4,\n",
        "    auto_augment=\"randaugment\",\n",
        "\n",
        "\n",
        "    # Training controls\n",
        "    patience=20,                   # Early stopping patience\n",
        "    val=True,                      # Run validation every epoch\n",
        "    save=True,                     # Save checkpoints\n",
        "    save_period=10,                # Save every 10 epochs\n",
        "    pretrained=True,               # Use pre-trained weights\n",
        "    workers=4,                     # Multi-threaded loading\n",
        "\n",
        "    # Logging & output\n",
        "    project=\"/content/drive/MyDrive/detection/yolo_results\",   # Folder for all experiments\n",
        "    name=\"exp_foodie_augmented\",  # Name of this training run\n",
        "    exist_ok=True,                # Overwrite if folder exists\n",
        "    verbose=True,\n",
        "    seed=42                       # Reproducibility\n",
        ")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- paths (edit if now differs) ---\n",
        "RUN_DIR = \"/content/drive/MyDrive/detection/yolo_results/exp_foodie_augmented\"\n",
        "LAST = f\"{RUN_DIR}/weights/last.pt\"   # your last.pt\n",
        "\n",
        "# sanity checks\n",
        "import os, sys\n",
        "assert os.path.exists(LAST), f\"Can't find: {LAST}\"\n",
        "assert os.path.exists(os.path.join(RUN_DIR, \"args.yaml\")), \\\n",
        "       f\"args.yaml not found next to weights in {RUN_DIR} (needed for resume)\"\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# load last checkpoint *from that run* and resume\n",
        "model = YOLO(LAST)\n",
        "model.train(resume=True)              # continues from epoch 48 in the same run folder\n",
        "\n",
        "# In case,  want to extend the total epochs (e.g., to 100 total):\n",
        "# model.train(resume=True, epochs=100)\n"
      ],
      "metadata": {
        "id": "jH5U9TpnHwM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# === CONFIG ===\n",
        "image_dir = \"/content/drive/MyDrive/detection/foodie/train/images\"\n",
        "label_dir = \"/content/drive/MyDrive/detection/foodie/train/labels\"\n",
        "samples_per_class = 150\n",
        "\n",
        "#  Target class name → class ID mapping (YOLOv8 format)\n",
        "class_name_to_id = {\n",
        "    'String Bean Chicken Breast': 5,\n",
        "    'chinese_sausage': 11,\n",
        "    'curry': 16,\n",
        "    'water_spinach': 21,\n",
        "    'tostitos cheese dip sauce': 22,\n",
        "    'mung_bean_sprouts': 20,\n",
        "    'black pepper rice bowl': 23\n",
        "}\n",
        "target_class_ids = set(class_name_to_id.values())\n",
        "\n",
        "# === AUGMENTATION PIPELINE ===\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Rotate(limit=15, p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.04, rotate_limit=15, p=0.6)\n",
        "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'], min_visibility=0.3))\n",
        "\n",
        "# === FUNCTION TO LOAD FILES BY CLASS ===\n",
        "def get_images_by_class():\n",
        "    class_to_images = {cls_id: [] for cls_id in target_class_ids}\n",
        "    for label_file in os.listdir(label_dir):\n",
        "        if not label_file.endswith(\".txt\"):\n",
        "            continue\n",
        "        label_path = os.path.join(label_dir, label_file)\n",
        "        try:\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    cls_id = int(float(line.strip().split()[0]))\n",
        "                    if cls_id in target_class_ids:\n",
        "                        class_to_images[cls_id].append(label_file.replace(\".txt\", \"\"))\n",
        "                        break\n",
        "        except:\n",
        "            continue\n",
        "    return class_to_images\n",
        "\n",
        "# === START AUGMENTATION ===\n",
        "class_to_images = get_images_by_class()\n",
        "augmented_total = 0\n",
        "\n",
        "print(\"\\n Starting YOLOv8-safe augmentation...\\n\")\n",
        "for cls_id, stems in class_to_images.items():\n",
        "    if len(stems) == 0:\n",
        "        print(f\" No images found for class {cls_id}. Skipping.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Augmenting class {cls_id}...\")\n",
        "    for i in tqdm(range(samples_per_class), desc=f\"Class {cls_id}\"):\n",
        "        stem = random.choice(stems)\n",
        "        img_path = os.path.join(image_dir, stem + \".jpg\")\n",
        "        lbl_path = os.path.join(label_dir, stem + \".txt\")\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        try:\n",
        "            with open(lbl_path, \"r\") as f:\n",
        "                lines = f.readlines()\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        bboxes = []\n",
        "        class_labels = []\n",
        "        for line in lines:\n",
        "            try:\n",
        "                parts = list(map(float, line.strip().split()))\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls, x, y, bw, bh = parts\n",
        "                # Convert to corner format to check\n",
        "                x_min = x - bw / 2\n",
        "                y_min = y - bh / 2\n",
        "                x_max = x + bw / 2\n",
        "                y_max = y + bh / 2\n",
        "                if not (0 <= x_min <= 1 and 0 <= y_min <= 1 and 0 <= x_max <= 1 and 0 <= y_max <= 1):\n",
        "                    continue\n",
        "                bboxes.append([x, y, bw, bh])\n",
        "                class_labels.append(int(cls))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if not bboxes:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "        aug_img = transformed['image']\n",
        "        aug_boxes = transformed['bboxes']\n",
        "        aug_cls = transformed['class_labels']\n",
        "\n",
        "        out_stem = f\"{stem}_aug{i}\"\n",
        "        out_img_path = os.path.join(image_dir, out_stem + \".jpg\")\n",
        "        out_lbl_path = os.path.join(label_dir, out_stem + \".txt\")\n",
        "\n",
        "        cv2.imwrite(out_img_path, aug_img)\n",
        "\n",
        "        with open(out_lbl_path, \"w\") as f:\n",
        "            for box, cls in zip(aug_boxes, aug_cls):\n",
        "                f.write(f\"{cls} {box[0]:.6f} {box[1]:.6f} {box[2]:.6f} {box[3]:.6f}\\n\")\n",
        "\n",
        "        augmented_total += 1\n",
        "\n",
        "print(f\"\\n Augmentation done! Total new images: {augmented_total}\")\n"
      ],
      "metadata": {
        "id": "ZAfcyohGQgMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yolo_v8_rebalance_train.py (Colab-ready paths)\n",
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Paths in Colab environment ---\n",
        "DATA = \"/content/foodie/data.yaml\"   # dataset YAML in foodie folder\n",
        "PROJECT = \"/content/yolo_results\"    # save results in /content/yolo_results\n",
        "RUN_NAME = \"exp_foodie_rebalance_v1\"\n",
        "START_WEIGHTS = \"/content/best.pt\"   # uploaded best.pt in /content\n",
        "results=\"/content/drive/MyDrive/detection/yolo_results\"\n",
        "def load_names(cfg):\n",
        "    names = cfg[\"names\"]\n",
        "    if isinstance(names, dict):\n",
        "        names = [names[i] for i in range(len(names))]\n",
        "    return names\n",
        "\n",
        "def count_classes(data_yaml_path: str):\n",
        "    with open(data_yaml_path, \"r\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    names = load_names(cfg)\n",
        "\n",
        "    labels_dir = Path(data_yaml_path).parent / \"train\" / \"labels\"\n",
        "    if not labels_dir.exists():\n",
        "        raise FileNotFoundError(f\"Labels dir not found: {labels_dir}\")\n",
        "\n",
        "    counts = Counter()\n",
        "    bad_lines = 0\n",
        "\n",
        "    for txt in labels_dir.glob(\"*.txt\"):\n",
        "        for raw in txt.read_text().splitlines():\n",
        "            line = raw.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            try:\n",
        "                cid = int(float(parts[0]))  # tolerate \"4.0\"\n",
        "            except Exception:\n",
        "                bad_lines += 1\n",
        "                continue\n",
        "            if 0 <= cid < len(names):\n",
        "                counts[cid] += 1\n",
        "            else:\n",
        "                bad_lines += 1\n",
        "\n",
        "    print(\"Class counts (train set):\")\n",
        "    for i, name in enumerate(names):\n",
        "        print(f\"{i:2d} {name:25s} -> {counts.get(i,0)}\")\n",
        "    if bad_lines:\n",
        "        print(f\"Skipped {bad_lines} malformed/out-of-range lines while counting.\")\n",
        "    return counts, names\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    counts, names = count_classes(DATA)\n",
        "\n",
        "    total = max(1, sum(counts.values()))\n",
        "    avg = total / max(1, len(names))\n",
        "    rare_ratio = sum(1 for i in range(len(names)) if counts.get(i, 0) < 0.7 * avg) / max(1, len(names))\n",
        "    cls_weight = 1.0 + min(0.6, 0.6 * rare_ratio)\n",
        "\n",
        "    print(f\"\\n Using cls loss weight = {cls_weight:.2f}  (rare_ratio={rare_ratio:.2f})\")\n",
        "    print(f\"Starting from weights: {START_WEIGHTS}\")\n",
        "\n",
        "    model = YOLO(START_WEIGHTS)\n",
        "\n",
        "    results = model.train(\n",
        "        data=DATA,\n",
        "        epochs=80,\n",
        "        imgsz=704,\n",
        "        batch=-1,\n",
        "        device=0,\n",
        "        workers=4,\n",
        "        cache='ram',\n",
        "\n",
        "        optimizer=\"AdamW\",\n",
        "        lr0=8e-4,\n",
        "        lrf=0.01,\n",
        "        cos_lr=True,\n",
        "        warmup_epochs=5,\n",
        "\n",
        "        box=7.5,\n",
        "        cls=cls_weight,\n",
        "        dfl=1.5,\n",
        "\n",
        "        mosaic=1.0,\n",
        "        close_mosaic=12,\n",
        "        mixup=0.25,\n",
        "        copy_paste=0.30,\n",
        "        erasing=0.35,\n",
        "        hsv_h=0.015, hsv_s=0.7, hsv_v=0.45,\n",
        "        degrees=5.0, translate=0.12, scale=0.5, shear=2.0,\n",
        "        fliplr=0.5, flipud=0.0,\n",
        "        perspective=0.0,\n",
        "\n",
        "        patience=30,\n",
        "        deterministic=True,\n",
        "        amp=True,\n",
        "\n",
        "        project=PROJECT,\n",
        "        name=RUN_NAME,\n",
        "        exist_ok=True,\n",
        "        save_period=10,\n",
        "        val=True,\n",
        "        pretrained=False,  # custom weights already loaded\n",
        "        plots=True,\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\n Done. Results at: {Path(results) / RUN_NAME}\")\n"
      ],
      "metadata": {
        "id": "MPXfTRZhVevv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "DATA = \"/content/foodie/data.yaml\"  # current yaml\n",
        "START = \"/content/best.pt\"\n",
        "\n",
        "model = YOLO(START)\n",
        "\n",
        "# train on all classes except 5\n",
        "keep = [i for i in range(24) if i != 5]\n",
        "\n",
        "results = model.train(\n",
        "    data=DATA,\n",
        "    classes=keep,          # <-- this excludes class 5 without editing files\n",
        "    epochs=60,\n",
        "    imgsz=736,\n",
        "    batch=-1,\n",
        "    device=0,\n",
        "    workers=4,\n",
        "    cache='ram',\n",
        "    optimizer=\"AdamW\",\n",
        "    lr0=8e-4,\n",
        "    lrf=0.01,\n",
        "    cos_lr=True,\n",
        "    warmup_epochs=3,\n",
        "    box=7.5, cls=1.20, dfl=1.5,\n",
        "    mosaic=1.0, close_mosaic=12,\n",
        "    mixup=0.15, copy_paste=0.30, erasing=0.35,\n",
        "    hsv_h=0.015, hsv_s=0.7, hsv_v=0.45,\n",
        "    degrees=5.0, translate=0.12, scale=0.5, shear=2.0,\n",
        "    fliplr=0.5, flipud=0.0,\n",
        "    patience=25,\n",
        "    deterministic=True,\n",
        "    amp=True,\n",
        "    project=\"/content/yolo_results\",\n",
        "    name=\"exp_foodie_no_stringbean\",\n",
        "    exist_ok=True,\n",
        "    save_period=10,\n",
        "    val=True,\n",
        "    pretrained=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "IbaEQuIyxZyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FINAL FIX: Remove String Bean Chicken Breast from model predictions ---\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# ====== CONFIG ======\n",
        "MODEL_PT   = \"/content/best.pt\"\n",
        "DATA_YAML  = \"/content/foodie/data.yaml\"\n",
        "PROJECT    = \"/content/yolo_results\"\n",
        "CONF, IOU, IMGSZ = 0.25, 0.50, 640\n",
        "RENDER_PREDS = True\n",
        "\n",
        "# The class to completely remove (it exists in model but not in yaml)\n",
        "REMOVE_CLASS_NAME = \"String Bean Chicken Breast\"\n",
        "REMOVE_CLASS_ID = 5  # Based on your debug output\n",
        "# ==================\n",
        "\n",
        "print(\"=== REMOVING STRING BEAN CHICKEN BREAST FROM ALL RESULTS ===\")\n",
        "\n",
        "# Load model\n",
        "model = YOLO(MODEL_PT)\n",
        "print(f\"Model has {len(model.names)} classes\")\n",
        "print(f\"Will remove class '{REMOVE_CLASS_NAME}' (ID: {REMOVE_CLASS_ID})\")\n",
        "\n",
        "# Load yaml\n",
        "dy_src = Path(DATA_YAML)\n",
        "with dy_src.open(\"r\") as f:\n",
        "    data = yaml.safe_load(f)\n",
        "\n",
        "yaml_classes = data.get(\"names\")\n",
        "if isinstance(yaml_classes, dict):\n",
        "    yaml_classes = [yaml_classes[i] for i in range(len(yaml_classes))]\n",
        "\n",
        "print(f\"YAML has {len(yaml_classes)} classes (String Bean removed)\")\n",
        "\n",
        "# Setup paths\n",
        "root = dy_src.parent\n",
        "test_images = root / \"test\" / \"images\"\n",
        "\n",
        "# Patch yaml for validation\n",
        "need_patch = False\n",
        "val_key = \"valid\" if \"valid\" in data else (\"val\" if \"val\" in data else None)\n",
        "if val_key is None:\n",
        "    data[\"valid\"] = str(test_images)\n",
        "    need_patch = True\n",
        "\n",
        "val_dir_path = Path(data[val_key]) if isinstance(data[val_key], str) else Path(str(data[val_key]))\n",
        "if not val_dir_path.exists():\n",
        "    data[val_key] = str(test_images)\n",
        "    need_patch = True\n",
        "\n",
        "if \"train\" not in data or not Path(data[\"train\"]).exists():\n",
        "    data[\"train\"] = str(test_images)\n",
        "    need_patch = True\n",
        "\n",
        "if need_patch:\n",
        "    dy_patched = dy_src.with_name(dy_src.stem + \"_patched.yaml\")\n",
        "    with dy_patched.open(\"w\") as f:\n",
        "        yaml.safe_dump(data, f)\n",
        "    data_yaml_for_eval = str(dy_patched)\n",
        "else:\n",
        "    data_yaml_for_eval = str(dy_src)\n",
        "\n",
        "# Clear caches\n",
        "for split_name in [\"train\", \"valid\", \"val\", \"test\"]:\n",
        "    cache_file = root / split_name / \"labels.cache\"\n",
        "    if cache_file.exists():\n",
        "        try:\n",
        "            cache_file.unlink()\n",
        "            print(f\"Cleared cache: {cache_file}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# === EVALUATION WITH STRING BEAN FILTERED OUT ===\n",
        "print(f\"\\n=== RUNNING EVALUATION (FILTERING OUT CLASS {REMOVE_CLASS_ID}) ===\")\n",
        "\n",
        "# Create list of classes to evaluate (exclude String Bean)\n",
        "all_class_ids = list(range(len(model.names)))\n",
        "classes_to_eval = [i for i in all_class_ids if i != REMOVE_CLASS_ID]\n",
        "\n",
        "print(f\"Evaluating {len(classes_to_eval)} classes (excluding String Bean)\")\n",
        "\n",
        "try:\n",
        "    metrics = model.val(\n",
        "        data=data_yaml_for_eval,\n",
        "        split=\"test\",\n",
        "        imgsz=IMGSZ,\n",
        "        conf=CONF,\n",
        "        iou=IOU,\n",
        "        classes=classes_to_eval,  # This excludes String Bean from metrics\n",
        "        plots=True,\n",
        "        project=PROJECT,\n",
        "        name=\"eval_no_string_bean\",\n",
        "        workers=4,\n",
        "        exist_ok=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- FILTERED EVALUATION RESULTS ---\")\n",
        "    print(f\"mAP50-95: {metrics.box.map:.3f}\")\n",
        "    print(f\"mAP50: {metrics.box.map50:.3f}\")\n",
        "    print(f\"Precision: {metrics.box.mp:.3f}\")\n",
        "    print(f\"Recall: {metrics.box.mr:.3f}\")\n",
        "    print(f\"String Bean Chicken Breast completely excluded from metrics\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Evaluation error: {e}\")\n",
        "\n",
        "# === CLEAN PREDICTIONS (NO STRING BEAN IN VISUALIZATIONS) ===\n",
        "if RENDER_PREDS:\n",
        "    print(f\"\\n=== GENERATING CLEAN VISUALIZATIONS (NO STRING BEAN) ===\")\n",
        "\n",
        "    save_dir = Path(PROJECT) / \"predictions_no_string_bean\"\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    test_files = list(test_images.glob(\"*.jpg\")) + list(test_images.glob(\"*.png\")) + list(test_images.glob(\"*.jpeg\"))\n",
        "\n",
        "    total_images = len(test_files)\n",
        "    string_bean_detections_removed = 0\n",
        "    images_with_string_bean = 0\n",
        "\n",
        "    print(f\"Processing {total_images} images...\")\n",
        "\n",
        "    for i, img_path in enumerate(test_files):\n",
        "        if i % 100 == 0 and i > 0:\n",
        "            print(f\"Processed {i}/{total_images} images\")\n",
        "\n",
        "        # Get predictions\n",
        "        results = model.predict(\n",
        "            source=str(img_path),\n",
        "            conf=CONF,\n",
        "            iou=IOU,\n",
        "            imgsz=IMGSZ,\n",
        "            save=False,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        if len(results) > 0:\n",
        "            result = results[0]\n",
        "\n",
        "            # Count and remove String Bean predictions\n",
        "            if result.boxes is not None and len(result.boxes) > 0:\n",
        "                original_count = len(result.boxes)\n",
        "\n",
        "                # Count String Bean detections\n",
        "                string_bean_mask = result.boxes.cls == REMOVE_CLASS_ID\n",
        "                string_bean_count = string_bean_mask.sum().item()\n",
        "\n",
        "                if string_bean_count > 0:\n",
        "                    images_with_string_bean += 1\n",
        "                    string_bean_detections_removed += string_bean_count\n",
        "                    print(f\"  {img_path.name}: Removed {string_bean_count} String Bean detection(s)\")\n",
        "\n",
        "                # Keep only non-String Bean predictions\n",
        "                keep_mask = result.boxes.cls != REMOVE_CLASS_ID\n",
        "\n",
        "                if keep_mask.any():\n",
        "                    # Filter all box attributes\n",
        "                    result.boxes.data = result.boxes.data[keep_mask]\n",
        "\n",
        "                    # Generate clean annotated image\n",
        "                    annotated = result.plot()\n",
        "                    output_path = save_dir / img_path.name\n",
        "                    cv2.imwrite(str(output_path), annotated)\n",
        "                else:\n",
        "                    # All detections were String Bean - save original image without annotations\n",
        "                    img = cv2.imread(str(img_path))\n",
        "                    output_path = save_dir / img_path.name\n",
        "                    cv2.imwrite(str(output_path), img)\n",
        "            else:\n",
        "                # No detections - save original\n",
        "                img = cv2.imread(str(img_path))\n",
        "                output_path = save_dir / img_path.name\n",
        "                cv2.imwrite(str(output_path), img)\n",
        "\n",
        "    print(f\"\\n--- CLEANING SUMMARY ---\")\n",
        "    print(f\" Processed {total_images} images\")\n",
        "    print(f\" {images_with_string_bean} images had String Bean detections\")\n",
        "    print(f\" Removed {string_bean_detections_removed} String Bean detection(s) total\")\n",
        "    print(f\" Clean images saved to: {save_dir}\")\n",
        "\n",
        "# === ANALYSIS: Show what we're filtering out ===\n",
        "print(f\"\\n=== ANALYSIS: What String Bean predictions look like ===\")\n",
        "\n",
        "# Test a bunch of images to find String Bean predictions\n",
        "test_files = list(test_images.glob(\"*.jpg\")) + list(test_images.glob(\"*.png\"))\n",
        "string_bean_examples = []\n",
        "\n",
        "for img_path in test_files[:50]:  # Check first 50 images\n",
        "    results = model.predict(source=str(img_path), conf=CONF, save=False, verbose=False)\n",
        "\n",
        "    if len(results) > 0 and results[0].boxes is not None:\n",
        "        boxes = results[0].boxes\n",
        "        string_bean_mask = boxes.cls == REMOVE_CLASS_ID\n",
        "\n",
        "        if string_bean_mask.any():\n",
        "            string_bean_confs = boxes.conf[string_bean_mask]\n",
        "            string_bean_examples.append({\n",
        "                'image': img_path.name,\n",
        "                'count': string_bean_mask.sum().item(),\n",
        "                'max_conf': string_bean_confs.max().item(),\n",
        "                'avg_conf': string_bean_confs.mean().item()\n",
        "            })\n",
        "\n",
        "if string_bean_examples:\n",
        "    print(f\"Found String Bean predictions in {len(string_bean_examples)} sample images:\")\n",
        "    for example in string_bean_examples[:5]:  # Show first 5\n",
        "        print(f\"  {example['image']}: {example['count']} detections, max_conf={example['max_conf']:.3f}\")\n",
        "\n",
        "    total_removed = sum(ex['count'] for ex in string_bean_examples)\n",
        "    avg_conf = np.mean([ex['avg_conf'] for ex in string_bean_examples])\n",
        "    print(f\"  Total String Bean detections found in sample: {total_removed}\")\n",
        "    print(f\"  Average confidence: {avg_conf:.3f}\")\n",
        "else:\n",
        "    print(\"No String Bean predictions found in sample images\")\n",
        "\n",
        "print(f\"\\n COMPLETE! String Bean Chicken Breast has been completely removed from:\")\n",
        "print(f\"    Evaluation metrics\")\n",
        "print(f\"    Visual predictions\")\n",
        "print(f\"    All outputs\")\n",
        "print(f\"\\n Our model results are now clean!\")\n"
      ],
      "metadata": {
        "id": "NUU8wAZYWoQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the results folder\n",
        "shutil.make_archive('/content/yolo_results', 'zip', '/content/yolo_results')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/yolo_results.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "qCvlAzjAzm_c",
        "outputId": "dacd07b8-8494-43f0-c2d7-ac89945af698",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_18412300-779e-4be0-973c-c380c4d4f7d6\", \"yolo_results.zip\", 53119049)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}